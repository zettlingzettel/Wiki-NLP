{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Work In Progress","metadata":{}},{"cell_type":"markdown","source":"License: Attribution-ShareAlike 4.0 International\nDataset: https://www.kaggle.com/datasets/wikimedia-foundation/wikipedia-structured-contents/data","metadata":{}},{"cell_type":"code","source":"# Install packages\n\n!pip install kagglehub\n!pip install tqdm\n!pip install scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Downloads the Wikipedia structured contents dataset from Kaggle.\n\nfile_path = kagglehub.dataset_download(\"wikimedia-foundation/wikipedia-structured-contents\",path=\"enwiki_namespace_0/enwiki_namespace_0_0.jsonl\")\ndf = pd.read_json(file_path, lines=True)\nprint(f\"Successfully loaded {len(df)} records\")\n\n# Saves it to the given path.\ndf.to_parquet('my-data-0_0.parquet', engine='pyarrow', compression='snappy')\n\n# Prints how many records (rows) were loaded.\nprint(\"Parquet file saved successfully!\")\n\n# Saves the DataFrame to a .parquet file (a compressed, fast format for large datasets).\ndf_read = pd.read_parquet('my-data-0_0.parquet')\n\n# Confirms that the file was saved.\nprint(df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nimport pandas as pd\n\n# Reads the Parquet file.\nread_parquet_df = pd.read_parquet(\"my-data-0_0.parquet\")\n\n# Converts the DataFrame to a JSON string.\nread_json_str_00 = read_parquet_df.to_json()\n\n# Saves that JSON string into a .csv file (even though it's JSON format — kind of a misnaming).\nwith open(\"data-0_0.csv\",\"w\") as file:\n    file.write( read_json_str_00 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # import libraries\nimport pandas as pd\n\n# # Reads the Parquet file.\nread_parquet_df = pd.read_parquet(\"my-data-0_0.parquet\")\n\n# # Converts the DataFrame to a JSON string.\n# read_json_str_00 = read_parquet_df.to_json()\nwith open(\"data-0_0.json\", \"w\") as f:\n    f.write(read_json_str_00)\n\n# # Saves that JSON string into a .csv file (even though it's JSON format — kind of a misnaming).\nwith open(\"data-0_0.csv\",\"w\") as file:\n    file.write( read_json_str_00 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nimport pandas as pd\nimport json\nimport csv\nfrom sklearn.model_selection import train_test_split\n\n# Load the JSON file into a Python dictionary\nwith open('data-0_0.json', 'r') as f:\n    data_dict = json.load(f)\n\n# Flatten the dictionary into a list\ndata_list = []\nfor key, value in data_dict.items():\n    if isinstance(value, list):\n        for item in value:\n            data_list.append({'Source': key, 'Target': item})\n    else:\n        data_list.append({'Source': key, 'Target': value})\n\n# Create a DataFrame\ndf = pd.DataFrame(data_list)\n\n# Splitting the dataframe into train data\ntrain_df , _ = train_test_split(df, test_size=0.0001, random_state=42)\nprint(train_df.head(10))\n\n\n\n\n# Select specific rows where 'Source' is 'sections'\nselected_rows = train_df[train_df['Source'] == 'sections']\n\n# test output\n# print(selected_rows.head(10))\n\ntext_list = []\n\n# Extract text from nested structure\nif not selected_rows.empty:\n    selected_target_rows = selected_rows['Target']\n\n    for entry in selected_target_rows:\n        if isinstance(entry, dict):  # <-- Always good to check\n            for key, sections in entry.items():\n                if isinstance(sections, list):  # Check if sections is a list\n                    for section in sections:\n                        has_parts = section.get('has_parts')\n                        if has_parts:\n                            for part in has_parts:\n                                value = part.get('value')\n                                if value is not None:\n                                    text_list.append(value)\n\n# Print first 2 results\nif text_list:\n    print(text_list[0])\n    print(text_list[1])\n    \n\n    with open('out.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerows(text_list)\nelse:\n    print(\"No text extracted.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import nltk\n# import re\n# import spacy\n# import pandas as pd\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\n# from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n\n# # Download required resources\n# nltk.download('punkt')\n# nltk.download('stopwords')\n# spacy.cli.download(\"en_core_web_lg\")\n# nlp = spacy.load(\"en_core_web_lg\")\n\n# STOPWORDS = set(stopwords.words('english'))\n\n# def clean_text(text):\n#     text = str(text).lower().strip()\n#     text = re.sub(r'\\s+', ' ', text)\n#     text = re.sub(r'#[\\w]+|@[\\w]+', lambda match: match.group(0), text)\n#     text = re.sub(r'[^a-zA-Z0-9\\s#@]', '', text)\n#     text = re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n#     return text\n\n# # Read CSV\n# df = pd.read_csv('out.csv', on_bad_lines='skip', dtype=str)\n# # Combine all character columns into one string per row\n# df['Extracted_Text'] = df.fillna('').astype(str).agg(''.join, axis=1)\n\n# # Sample 10,000 rows\n# df_sample = df.sample(n=10000, random_state=42)\n\n# # Clean text\n# df_sample['Cleaned_Extracted_Text'] = df_sample['Extracted_Text'].apply(clean_text)\n\n# def tokenize_and_remove_stopwords(text):\n#     tokens = word_tokenize(text)\n#     return [word for word in tokens if word not in STOPWORDS]\n\n# # Tokenize and remove stopwords\n# df_sample['Tokens'] = df_sample['Cleaned_Extracted_Text'].apply(tokenize_and_remove_stopwords)\n\n# # Combine all tokens\n# all_words = []\n# for tokens in df_sample['Tokens']:\n#     all_words.extend(tokens)\n\n# # Find bigrams\n# bigram_measures = BigramAssocMeasures()\n# finder = BigramCollocationFinder.from_words(all_words)\n# finder.apply_freq_filter(2)\n\n# top_bigrams = finder.nbest(bigram_measures.raw_freq, 10)\n# top_bigrams_500 = finder.nbest(bigram_measures.raw_freq, 500)\n\n\n# # Output\n# print(df_sample[['Cleaned_Extracted_Text','Tokens']].head(10))\n# print(\"Rows after sampling:\", len(df_sample))\n# print(\"Top 10 bigrams by raw freq:\", top_bigrams)\n# print(\"Top 10 bigrams by raw freq:\", top_bigrams_500)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nimport re\nimport spacy\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\nfrom collections import Counter\n\n# Download required resources\nnltk.download('punkt')\nnltk.download('stopwords')\nspacy.cli.download(\"en_core_web_lg\")\nnlp = spacy.load(\"en_core_web_lg\")\n\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = str(text).lower().strip()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'#[\\w]+|@[\\w]+', lambda match: match.group(0), text)\n    text = re.sub(r'[^a-zA-Z0-9\\s#@]', '', text)\n    text = re.sub(r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n    return text\n\n# Read CSV\ndf = pd.read_csv('out.csv', on_bad_lines='skip', dtype=str)\n# Combine all character columns into one string per row\ndf['Extracted_Text'] = df.fillna('').astype(str).agg(''.join, axis=1)\n\n# Sample 10,000 rows\ndf_sample = df.sample(n=10000, random_state=42)\n\n# Clean text\ndf_sample['Cleaned_Extracted_Text'] = df_sample['Extracted_Text'].apply(clean_text)\n\ndef tokenize_and_remove_stopwords(text):\n    tokens = word_tokenize(text)\n    return [word for word in tokens if word not in STOPWORDS]\n\n# Tokenize and remove stopwords\ndf_sample['Tokens'] = df_sample['Cleaned_Extracted_Text'].apply(tokenize_and_remove_stopwords)\n\n# Combine all tokens\nall_words = []\nfor tokens in df_sample['Tokens']:\n    all_words.extend(tokens)\n\n# Find bigrams\nbigram_measures = BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_words(all_words)\nfinder.apply_freq_filter(2)\n\ntop_bigrams = finder.nbest(bigram_measures.raw_freq, 10)\ntop_bigrams_500 = finder.nbest(bigram_measures.raw_freq, 500)\n\n# Count frequencies\nword_freq = Counter(all_words)\n\n# Get most common words\nmost_common_words = word_freq.most_common(100)  \n\n# Create a set of common words\ncommon_words_set = set(word for word, _ in most_common_words)\n\n# Output\nprint(df_sample[['Cleaned_Extracted_Text','Tokens']].head(10))\nprint(\"Rows after sampling:\", len(df_sample))\nprint(\"Top 10 bigrams by raw freq:\", top_bigrams)\nprint(\"Top 10 bigrams by raw freq:\", top_bigrams_500)\nprint(most_common_words)\n\nmost_common_words_in_bigrams = []\nfor bigram in top_bigrams_500:\n    most_common_words_in_bigrams.append((bigram[0], bigram[1]))  # Fix: Append as a tuple\n\nfiltered_bigrams_partially_common = [\n    bigram for bigram in top_bigrams_500 \n    if bigram[0] in common_words_set or bigram[1] in common_words_set\n]\n\nprint(\"Filtered bigrams (at least one word common):\", filtered_bigrams_partially_common[:20])\n\nall_pmi_bigrams = finder.score_ngrams(bigram_measures.pmi)\n\n# Print them\nprint(\"All bigrams scored by PMI:\")\nfor bigram, score in all_pmi_bigrams:\n    print(bigram, score)\n\n# Likelihood Ratio:\nall_lr_bigrams = finder.score_ngrams(bigram_measures.likelihood_ratio)\n\nprint(\"\\nAll bigrams scored by Likelihood Ratio:\")\nfor bigram, score in all_lr_bigrams:\n    print(bigram, score)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}